{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1181356,"sourceType":"datasetVersion","datasetId":671172}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install --upgrade protobuf\n!pip install tensorflow --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nFEATURE_FLAGS = {\n    # Model Architecture Features\n    'USE_CBAM_ATTENTION': True,          # Enable/disable CBAM attention blocks\n    'USE_SPP_MODULE': True,              # Enable/disable Spatial Pyramid Pooling\n    'USE_RESIDUAL_BLOCKS': True,         # Enable/disable residual connections\n    'USE_MISH_ACTIVATION': False,        # Use Mish instead of LeakyReLU\n    \n    # Training Features\n    'USE_DATA_AUGMENTATION': True,       # Enable/disable data augmentation\n    'USE_MIXED_PRECISION': False,        # Enable mixed precision training\n    'USE_GRADIENT_CLIPPING': True,       # Enable gradient clipping\n    \n    # Loss Function Features\n    'USE_FOCAL_LOSS': False,             # Use focal loss for classification\n    'USE_GIOU_LOSS': False,              # Use GIoU loss instead of MSE for boxes\n    'USE_LABEL_SMOOTHING': True,         # Apply label smoothing\n    \n    # Post-processing Features\n    'USE_SOFT_NMS': False,               # Use Soft-NMS instead of standard NMS\n    'USE_MULTISCALE_INFERENCE': False,   # Test-time augmentation with multiple scales\n    \n    # Monitoring & Debug Features\n    'VERBOSE_LOGGING': True,             # Extra logging during training\n    'SAVE_INTERMEDIATE_OUTPUTS': False,  # Save feature maps for visualization\n    'EARLY_STOPPING_ENABLED': True,      # Enable early stopping\n    'SAVE_BEST_ONLY': True,              # Only save best model checkpoint\n}\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nimport xml.etree.ElementTree as ET\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if FEATURE_FLAGS['USE_MIXED_PRECISION']:\n    from tensorflow.keras import mixed_precision\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    print(\"Mixed precision enabled\")\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONFIG = {\n    'IMAGE_SIZE': 416,\n    'GRID_SIZE': 13,\n    'NUM_CLASSES': 4,\n    'NUM_ANCHORS': 5,\n    'BATCH_SIZE': 8,\n    'EPOCHS': 100,\n    'LEARNING_RATE': 1e-4,\n    'GRADIENT_CLIP_VALUE': 1.0 if FEATURE_FLAGS['USE_GRADIENT_CLIPPING'] else None,\n}\n\nCLASS_NAMES = ['trafficlight', 'stop', 'speedlimit', 'crosswalk']\nCLASS_TO_IDX = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n\n# Anchor boxes (width, height) normalized\nANCHORS = np.array([\n    [0.1, 0.2], [0.2, 0.4], [0.3, 0.3], [0.4, 0.6], [0.5, 0.5]\n], dtype=np.float32)\n\n# Dataset paths - UPDATE THESE FOR YOUR ENVIRONMENT\nIMAGES_DIR = '/kaggle/input/road-sign-detection/images'\nANNOTATIONS_DIR = '/kaggle/input/road-sign-detection/annotations'\n\nprint(\"Configuration loaded!\")\nprint(f\"Active Feature Flags:\")\nfor flag, value in FEATURE_FLAGS.items():\n    if value:\n        print(f\"  âœ“ {flag}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_voc_annotation(xml_path):\n    \"\"\"Parse PASCAL VOC XML annotation file.\"\"\"\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    \n    size = root.find('size')\n    width = int(size.find('width').text)\n    height = int(size.find('height').text)\n    \n    objects = []\n    for obj in root.findall('object'):\n        name = obj.find('name').text.lower().replace(' ', '')\n        if name not in CLASS_TO_IDX:\n            continue\n        bbox = obj.find('bndbox')\n        xmin = int(bbox.find('xmin').text)\n        ymin = int(bbox.find('ymin').text)\n        xmax = int(bbox.find('xmax').text)\n        ymax = int(bbox.find('ymax').text)\n        \n        # Normalize coordinates\n        xmin_n = max(0, min(1, xmin / width))\n        ymin_n = max(0, min(1, ymin / height))\n        xmax_n = max(0, min(1, xmax / width))\n        ymax_n = max(0, min(1, ymax / height))\n        \n        # Ensure valid box\n        if xmax_n > xmin_n and ymax_n > ymin_n:\n            objects.append({\n                'class_idx': CLASS_TO_IDX[name],\n                'bbox': [xmin_n, ymin_n, xmax_n, ymax_n]\n            })\n    \n    return objects, width, height","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_dataset(images_dir, annotations_dir):\n    \"\"\"Load all images and annotations.\"\"\"\n    data = []\n    \n    if not os.path.exists(annotations_dir):\n        print(f\"Error: Annotations directory not found: {annotations_dir}\")\n        return data\n    \n    if not os.path.exists(images_dir):\n        print(f\"Error: Images directory not found: {images_dir}\")\n        return data\n    \n    ann_files = [f for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n    \n    for ann_file in ann_files:\n        ann_path = os.path.join(annotations_dir, ann_file)\n        try:\n            objects, orig_w, orig_h = parse_voc_annotation(ann_path)\n        except Exception as e:\n            if FEATURE_FLAGS['VERBOSE_LOGGING']:\n                print(f\"Error parsing {ann_file}: {e}\")\n            continue\n        \n        if not objects:\n            continue\n        \n        base_name = os.path.splitext(ann_file)[0]\n        img_path = None\n        for ext in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n            candidate = os.path.join(images_dir, base_name + ext)\n            if os.path.exists(candidate):\n                img_path = candidate\n                break\n        \n        if img_path is None:\n            continue\n        \n        data.append({\n            'image_path': img_path,\n            'objects': objects,\n            'orig_size': (orig_w, orig_h)\n        })\n    \n    print(f\"Loaded {len(data)} samples\")\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_target_tensor(objects, grid_size, num_anchors, num_classes, anchors):\n    \"\"\"Create YOLO-style target tensor.\"\"\"\n    target = np.zeros((grid_size, grid_size, num_anchors, 5 + num_classes), dtype=np.float32)\n    \n    for obj in objects:\n        xmin, ymin, xmax, ymax = obj['bbox']\n        class_idx = obj['class_idx']\n        \n        cx = (xmin + xmax) / 2\n        cy = (ymin + ymax) / 2\n        w = xmax - xmin\n        h = ymax - ymin\n        \n        if w <= 0 or h <= 0:\n            continue\n        \n        grid_x = int(cx * grid_size)\n        grid_y = int(cy * grid_size)\n        grid_x = min(grid_x, grid_size - 1)\n        grid_y = min(grid_y, grid_size - 1)\n        \n        # Find best anchor\n        best_iou = 0\n        best_anchor = 0\n        for i, anchor in enumerate(anchors):\n            anchor_w, anchor_h = anchor\n            if anchor_w <= 0 or anchor_h <= 0:\n                continue\n            intersection = min(w, anchor_w) * min(h, anchor_h)\n            union = w * h + anchor_w * anchor_h - intersection\n            iou = intersection / (union + 1e-8)\n            if iou > best_iou:\n                best_iou = iou\n                best_anchor = i\n        \n        # Set target values\n        target[grid_y, grid_x, best_anchor, 0] = cx * grid_size - grid_x\n        target[grid_y, grid_x, best_anchor, 1] = cy * grid_size - grid_y\n        target[grid_y, grid_x, best_anchor, 2] = np.log(w / (anchors[best_anchor][0] + 1e-8) + 1e-8)\n        target[grid_y, grid_x, best_anchor, 3] = np.log(h / (anchors[best_anchor][1] + 1e-8) + 1e-8)\n        target[grid_y, grid_x, best_anchor, 4] = 1.0\n        \n        # Label smoothing\n        if FEATURE_FLAGS['USE_LABEL_SMOOTHING']:\n            smooth = 0.1\n            target[grid_y, grid_x, best_anchor, 5 + class_idx] = 1.0 - smooth\n            target[grid_y, grid_x, best_anchor, 5:] += smooth / num_classes\n        else:\n            target[grid_y, grid_x, best_anchor, 5 + class_idx] = 1.0\n    \n    return target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ObjectDetectionDataset(keras.utils.Sequence):\n    def __init__(self, data, config, anchors, augment=False):\n        self.data = data\n        self.config = config\n        self.anchors = anchors\n        self.augment = augment and FEATURE_FLAGS['USE_DATA_AUGMENTATION']\n        self.indices = np.arange(len(data))\n    \n    def __len__(self):\n        return max(1, len(self.data) // self.config['BATCH_SIZE'])\n    \n    def __getitem__(self, idx):\n        batch_size = self.config['BATCH_SIZE']\n        start_idx = idx * batch_size\n        end_idx = min(start_idx + batch_size, len(self.data))\n        batch_indices = self.indices[start_idx:end_idx]\n        \n        images = []\n        targets = []\n        \n        for i in batch_indices:\n            sample = self.data[i]\n            \n            img = cv2.imread(sample['image_path'])\n            if img is None:\n                img = np.zeros((self.config['IMAGE_SIZE'], self.config['IMAGE_SIZE'], 3), dtype=np.uint8)\n            else:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                img = cv2.resize(img, (self.config['IMAGE_SIZE'], self.config['IMAGE_SIZE']))\n            \n            objects = [obj.copy() for obj in sample['objects']]\n            \n            if self.augment and len(objects) > 0:\n                # Horizontal flip\n                if np.random.random() > 0.5:\n                    img = np.fliplr(img).copy()\n                    for obj in objects:\n                        xmin, ymin, xmax, ymax = obj['bbox']\n                        obj['bbox'] = [1 - xmax, ymin, 1 - xmin, ymax]\n                \n                # Brightness augmentation\n                img = img.astype(np.float32)\n                img *= np.random.uniform(0.8, 1.2)\n                img = np.clip(img, 0, 255).astype(np.uint8)\n            \n            img = img.astype(np.float32) / 255.0\n            \n            target = create_target_tensor(\n                objects, self.config['GRID_SIZE'], \n                self.config['NUM_ANCHORS'], self.config['NUM_CLASSES'],\n                self.anchors\n            )\n            \n            images.append(img)\n            targets.append(target)\n        \n        return np.array(images), np.array(targets)\n    \n    def on_epoch_end(self):\n        if self.augment:\n            np.random.shuffle(self.indices)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv_block(x, filters, kernel_size=3, strides=1, use_bn=True, activation='leaky'):\n    x = layers.Conv2D(filters, kernel_size, strides=strides, padding='same', use_bias=not use_bn)(x)\n    if use_bn:\n        x = layers.BatchNormalization()(x)\n    \n    if FEATURE_FLAGS['USE_MISH_ACTIVATION'] and activation in ['leaky', 'mish']:\n        # Mish activation: x * tanh(softplus(x))\n        x = layers.Lambda(lambda t: t * tf.nn.tanh(tf.nn.softplus(t)))(x)\n    elif activation == 'leaky':\n        x = layers.LeakyReLU(0.1)(x)\n    \n    return x\n\ndef residual_block(x, filters):\n    if not FEATURE_FLAGS['USE_RESIDUAL_BLOCKS']:\n        x = conv_block(x, filters // 2, kernel_size=1)\n        x = conv_block(x, filters, kernel_size=3)\n        return x\n    \n    shortcut = x\n    x = conv_block(x, filters // 2, kernel_size=1)\n    x = conv_block(x, filters, kernel_size=3)\n    if shortcut.shape[-1] != filters:\n        shortcut = conv_block(shortcut, filters, kernel_size=1, use_bn=False, activation=None)\n    x = layers.Add()([shortcut, x])\n    x = layers.LeakyReLU(0.1)(x)\n    return x\n\ndef spatial_attention(x):\n    avg_pool = layers.Lambda(lambda t: tf.reduce_mean(t, axis=-1, keepdims=True))(x)\n    max_pool = layers.Lambda(lambda t: tf.reduce_max(t, axis=-1, keepdims=True))(x)\n    concat = layers.Concatenate()([avg_pool, max_pool])\n    attention = layers.Conv2D(1, 7, padding='same', activation='sigmoid')(concat)\n    return layers.Multiply()([x, attention])\n\ndef channel_attention(x, ratio=8):\n    channels = x.shape[-1]\n    \n    # Use Keras pooling layers\n    avg_pool = layers.GlobalAveragePooling2D()(x)\n    max_pool = layers.GlobalMaxPooling2D()(x)\n    \n    shared_dense1 = layers.Dense(channels // ratio, activation='relu')\n    shared_dense2 = layers.Dense(channels)\n    \n    avg_out = shared_dense2(shared_dense1(avg_pool))\n    max_out = shared_dense2(shared_dense1(max_pool))\n    \n    attention = layers.Add()([avg_out, max_out])\n    attention = layers.Activation('sigmoid')(attention)\n    attention = layers.Reshape((1, 1, channels))(attention)\n    return layers.Multiply()([x, attention])\n\ndef cbam_block(x, ratio=8):\n    \"\"\"Convolutional Block Attention Module\"\"\"\n    if not FEATURE_FLAGS['USE_CBAM_ATTENTION']:\n        return x\n    \n    x = channel_attention(x, ratio)\n    x = spatial_attention(x)\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_detector(config, anchors):\n    inputs = layers.Input(shape=(config['IMAGE_SIZE'], config['IMAGE_SIZE'], 3))\n    \n    # Stem\n    x = conv_block(inputs, 32, kernel_size=3, strides=1)\n    x = conv_block(x, 64, kernel_size=3, strides=2)  # 208x208\n    \n    # Stage 1\n    x = residual_block(x, 64)\n    x = conv_block(x, 128, kernel_size=3, strides=2)  # 104x104\n    \n    # Stage 2\n    for _ in range(2):\n        x = residual_block(x, 128)\n    x = cbam_block(x)\n    x = conv_block(x, 256, kernel_size=3, strides=2)  # 52x52\n    \n    # Stage 3\n    for _ in range(4):\n        x = residual_block(x, 256)\n    x = cbam_block(x)\n    x = conv_block(x, 512, kernel_size=3, strides=2)  # 26x26\n    \n    # Stage 4\n    for _ in range(4):\n        x = residual_block(x, 512)\n    x = cbam_block(x)\n    x = conv_block(x, 1024, kernel_size=3, strides=2)  # 13x13\n    \n    # Stage 5\n    for _ in range(2):\n        x = residual_block(x, 1024)\n    x = cbam_block(x)\n    \n    # SPP Module (optional)\n    if FEATURE_FLAGS['USE_SPP_MODULE']:\n        pool1 = layers.MaxPooling2D(5, strides=1, padding='same')(x)\n        pool2 = layers.MaxPooling2D(9, strides=1, padding='same')(x)\n        pool3 = layers.MaxPooling2D(13, strides=1, padding='same')(x)\n        x = layers.Concatenate()([x, pool1, pool2, pool3])\n        x = conv_block(x, 1024, kernel_size=1)\n    \n    # Detection Head\n    x = conv_block(x, 512, kernel_size=1)\n    x = conv_block(x, 1024, kernel_size=3)\n    x = conv_block(x, 512, kernel_size=1)\n    x = conv_block(x, 1024, kernel_size=3)\n    \n    output_channels = config['NUM_ANCHORS'] * (5 + config['NUM_CLASSES'])\n    outputs = layers.Conv2D(output_channels, 1, padding='same')(x)\n    outputs = layers.Reshape((config['GRID_SIZE'], config['GRID_SIZE'], \n                              config['NUM_ANCHORS'], 5 + config['NUM_CLASSES']))(outputs)\n    \n    model = Model(inputs, outputs, name='CustomDetector')\n    return model\n\n# Build model\nmodel = build_detector(CONFIG, ANCHORS)\nprint(\"\\nModel built successfully!\")\nprint(f\"Total parameters: {model.count_params():,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DetectionLoss(keras.losses.Loss):\n    def __init__(self, config, anchors, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.anchors = tf.constant(anchors, dtype=tf.float32)\n        self.lambda_coord = 5.0\n        self.lambda_noobj = 0.5\n        self.lambda_obj = 1.0\n        self.lambda_class = 1.0\n    \n    def call(self, y_true, y_pred):\n        true_xy = y_true[..., :2]\n        true_wh = y_true[..., 2:4]\n        true_obj = y_true[..., 4:5]\n        true_class = y_true[..., 5:]\n        \n        pred_xy = tf.sigmoid(y_pred[..., :2])\n        pred_wh = y_pred[..., 2:4]\n        pred_obj = tf.sigmoid(y_pred[..., 4:5])\n        pred_class = tf.sigmoid(y_pred[..., 5:])\n        \n        obj_mask = true_obj\n        noobj_mask = 1 - obj_mask\n        \n        xy_loss = obj_mask * tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1, keepdims=True)\n        wh_loss = obj_mask * tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1, keepdims=True)\n        coord_loss = self.lambda_coord * tf.reduce_sum(xy_loss + wh_loss)\n        \n        if FEATURE_FLAGS['USE_FOCAL_LOSS']:\n            alpha = 0.25\n            gamma = 2.0\n            obj_bce = tf.keras.losses.binary_crossentropy(true_obj, pred_obj)\n            obj_focal = alpha * tf.pow(1 - pred_obj, gamma) * obj_bce\n            obj_loss = self.lambda_obj * tf.reduce_sum(obj_mask * obj_focal)\n            noobj_loss = self.lambda_noobj * tf.reduce_sum(noobj_mask * obj_focal)\n        else:\n            obj_loss = self.lambda_obj * tf.reduce_sum(obj_mask * tf.square(1 - pred_obj))\n            noobj_loss = self.lambda_noobj * tf.reduce_sum(noobj_mask * tf.square(pred_obj))\n        \n        class_loss = self.lambda_class * tf.reduce_sum(\n            obj_mask * tf.reduce_sum(tf.square(true_class - pred_class), axis=-1, keepdims=True)\n        )\n        \n        total_loss = coord_loss + obj_loss + noobj_loss + class_loss\n        batch_size = tf.cast(tf.shape(y_true)[0], tf.float32)\n        \n        return total_loss / (batch_size + 1e-8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_predictions(predictions, config, anchors, conf_threshold=0.5, nms_threshold=0.4):\n    batch_size = predictions.shape[0]\n    grid_size = config['GRID_SIZE']\n    \n    all_boxes = []\n    \n    for b in range(batch_size):\n        boxes = []\n        pred = predictions[b]\n        \n        for gy in range(grid_size):\n            for gx in range(grid_size):\n                for a in range(config['NUM_ANCHORS']):\n                    objectness = 1 / (1 + np.exp(-np.clip(pred[gy, gx, a, 4], -10, 10)))\n                    \n                    if objectness < conf_threshold:\n                        continue\n                    \n                    class_probs = 1 / (1 + np.exp(-np.clip(pred[gy, gx, a, 5:], -10, 10)))\n                    class_idx = np.argmax(class_probs)\n                    class_conf = class_probs[class_idx]\n                    \n                    confidence = objectness * class_conf\n                    if confidence < conf_threshold:\n                        continue\n                    \n                    cx = (gx + 1 / (1 + np.exp(-np.clip(pred[gy, gx, a, 0], -10, 10)))) / grid_size\n                    cy = (gy + 1 / (1 + np.exp(-np.clip(pred[gy, gx, a, 1], -10, 10)))) / grid_size\n                    w = anchors[a, 0] * np.exp(np.clip(pred[gy, gx, a, 2], -10, 10))\n                    h = anchors[a, 1] * np.exp(np.clip(pred[gy, gx, a, 3], -10, 10))\n                    \n                    xmin = max(0, cx - w / 2)\n                    ymin = max(0, cy - h / 2)\n                    xmax = min(1, cx + w / 2)\n                    ymax = min(1, cy + h / 2)\n                    \n                    boxes.append([xmin, ymin, xmax, ymax, confidence, class_idx])\n        \n        if boxes:\n            boxes = np.array(boxes)\n            if FEATURE_FLAGS['USE_SOFT_NMS']:\n                boxes = apply_soft_nms(boxes, nms_threshold)\n            else:\n                boxes = apply_nms(boxes, nms_threshold)\n        else:\n            boxes = np.array([]).reshape(0, 6)\n        \n        all_boxes.append(boxes)\n    \n    return all_boxes\n\ndef apply_nms(boxes, threshold):\n    if len(boxes) == 0:\n        return boxes\n    \n    order = boxes[:, 4].argsort()[::-1]\n    boxes = boxes[order]\n    \n    keep = []\n    while len(boxes) > 0:\n        keep.append(boxes[0])\n        if len(boxes) == 1:\n            break\n        \n        box = boxes[0, :4]\n        other_boxes = boxes[1:, :4]\n        \n        x1 = np.maximum(box[0], other_boxes[:, 0])\n        y1 = np.maximum(box[1], other_boxes[:, 1])\n        x2 = np.minimum(box[2], other_boxes[:, 2])\n        y2 = np.minimum(box[3], other_boxes[:, 3])\n        \n        intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n        box_area = (box[2] - box[0]) * (box[3] - box[1])\n        other_areas = (other_boxes[:, 2] - other_boxes[:, 0]) * (other_boxes[:, 3] - other_boxes[:, 1])\n        iou = intersection / (box_area + other_areas - intersection + 1e-8)\n        \n        boxes = boxes[1:][iou < threshold]\n    \n    return np.array(keep)\n\ndef apply_soft_nms(boxes, threshold, sigma=0.5):\n    \"\"\"Soft-NMS implementation\"\"\"\n    if len(boxes) == 0:\n        return boxes\n    \n    boxes = boxes.copy()\n    keep = []\n    \n    while len(boxes) > 0:\n        idx = boxes[:, 4].argmax()\n        keep.append(boxes[idx])\n        \n        if len(boxes) == 1:\n            break\n        \n        box = boxes[idx, :4]\n        other_boxes = np.delete(boxes, idx, axis=0)\n        \n        x1 = np.maximum(box[0], other_boxes[:, 0])\n        y1 = np.maximum(box[1], other_boxes[:, 1])\n        x2 = np.minimum(box[2], other_boxes[:, 2])\n        y2 = np.minimum(box[3], other_boxes[:, 3])\n        \n        intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n        box_area = (box[2] - box[0]) * (box[3] - box[1])\n        other_areas = (other_boxes[:, 2] - other_boxes[:, 0]) * (other_boxes[:, 3] - other_boxes[:, 1])\n        iou = intersection / (box_area + other_areas - intersection + 1e-8)\n        other_boxes[:, 4] *= np.exp(-(iou ** 2) / sigma)\n        boxes = other_boxes\n        boxes = boxes[boxes[:, 4] > threshold * 0.1]\n    return np.array(keep)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_predictions(image, boxes, class_names, save_path=None):\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n    ax.imshow(image)\n    \n    colors = ['red', 'blue', 'green', 'yellow']\n    \n    h, w = image.shape[:2]\n    for box in boxes:\n        xmin, ymin, xmax, ymax, conf, class_idx = box\n        class_idx = int(class_idx)\n        \n        rect = patches.Rectangle(\n            (xmin * w, ymin * h), (xmax - xmin) * w, (ymax - ymin) * h,\n            linewidth=2, edgecolor=colors[class_idx], facecolor='none'\n        )\n        ax.add_patch(rect)\n        \n        label = f\"{class_names[class_idx]}: {conf:.2f}\"\n        ax.text(xmin * w, ymin * h - 5, label, \n                color='white', fontsize=10, weight='bold',\n                bbox=dict(boxstyle='round', facecolor=colors[class_idx], alpha=0.8))\n    \n    ax.axis('off')\n    \n    if save_path:\n        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_sample(data, idx):\n    sample = data[idx]\n    img = cv2.imread(sample['image_path'])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    boxes = []\n    for obj in sample['objects']:\n        boxes.append(obj['bbox'] + [1.0, obj['class_idx']])\n    \n    visualize_predictions(img, boxes, CLASS_NAMES)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading dataset...\")\ndata = load_dataset(IMAGES_DIR, ANNOTATIONS_DIR)\n\nif len(data) == 0:\n    print(\"\\n\" + \"=\"*60)\n    print(\"ERROR: No data found!\")\n    print(\"=\"*60)\n    print(\"\\nPlease update paths in CELL 3\")\nelse:\n    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n    print(f\"\\nTrain: {len(train_data)}, Validation: {len(val_data)}\")\n    \n    train_gen = ObjectDetectionDataset\n# Create data generators\ntrain_gen = ObjectDetectionDataset(train_data, CONFIG, ANCHORS, augment=True)\nval_gen = ObjectDetectionDataset(val_data, CONFIG, ANCHORS, augment=False)\n\n# Compile model\noptimizer = keras.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE'])\nif FEATURE_FLAGS['USE_GRADIENT_CLIPPING']:\n    optimizer = keras.optimizers.Adam(\n        learning_rate=CONFIG['LEARNING_RATE'],\n        clipvalue=CONFIG['GRADIENT_CLIP_VALUE']\n    )\nloss_fn = DetectionLoss(CONFIG, ANCHORS)\nmodel.compile(optimizer=optimizer, loss=loss_fn)\n\n# Callbacks\ncallbacks = [\n    keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1\n    ),\n    keras.callbacks.ModelCheckpoint(\n        'best_detector.keras', monitor='val_loss', save_best_only=FEATURE_FLAGS['SAVE_BEST_ONLY'], verbose=1\n    ),\n]\nif FEATURE_FLAGS['EARLY_STOPPING_ENABLED']:\n    callbacks.append(\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n        )\n    )\n\n# Train\nprint(\"\\n\" + \"=\"*60)\nprint(\"Starting training...\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=CONFIG['EPOCHS'],\n    callbacks=callbacks,\n    verbose=1\n)\nprint(\"Training completed!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"final_detector.keras\")\nprint(\"Final model saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random, math\n\ndef predict_and_show_grid(model, data, n=9, conf=0.1):\n    # sample indices instead of samples\n    indices = random.sample(range(len(data)), n)\n    \n    cols = 3\n    rows = math.ceil(n / cols)\n    fig, axes = plt.subplots(rows, cols, figsize=(14, 10))\n    axes = axes.flatten()\n\n    for ax, idx in zip(axes, indices):\n        sample = data[idx]\n\n        img = cv2.imread(sample['image_path'])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        resized = cv2.resize(img, (CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']))\n        input_tensor = resized.astype(np.float32) / 255.0\n        input_tensor = np.expand_dims(input_tensor, axis=0)\n\n        preds = model.predict(input_tensor, verbose=0)\n        boxes = decode_predictions(preds, CONFIG, ANCHORS,\n                                   conf_threshold=conf,\n                                   nms_threshold=0.45)[0]\n\n        ax.imshow(resized)\n        h, w = resized.shape[:2]\n\n        for box in boxes:\n            xmin, ymin, xmax, ymax, score, cls = box\n            cls = int(cls)\n\n            rect = patches.Rectangle(\n                (xmin * w, ymin * h),\n                (xmax - xmin) * w,\n                (ymax - ymin) * h,\n                linewidth=2,\n                edgecolor='red',\n                facecolor='none'\n            )\n            ax.add_patch(rect)\n\n            ax.text(xmin * w, ymin * h - 5,\n                    f\"{CLASS_NAMES[cls]} {score:.2f}\",\n                    color='white', fontsize=8,\n                    bbox=dict(facecolor='red', alpha=0.7))\n\n        # ðŸ‘‰ Show index on the image\n        ax.text(5, 20, f\"Index: {idx}\",\n                color='yellow', fontsize=11, weight='bold',\n                bbox=dict(facecolor='black', alpha=0.7))\n\n        ax.axis('off')\n\n    for ax in axes[len(indices):]:\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\npredict_and_show_grid(model, val_data, n=9, conf=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_gt_vs_prediction_fixed(model, data, indices, conf=0.1):\n    n = len(indices)\n    fig, axes = plt.subplots(n, 2, figsize=(10, 4*n))\n\n    for row, idx in enumerate(indices):\n        sample = data[idx]\n\n        # ---- Load image ----\n        img = cv2.imread(sample['image_path'])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(img, (CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']))\n\n        # ---- Prediction ----\n        input_tensor = resized.astype(np.float32) / 255.0\n        input_tensor = np.expand_dims(input_tensor, axis=0)\n        preds = model.predict(input_tensor, verbose=0)\n\n        pred_boxes = decode_predictions(preds, CONFIG, ANCHORS,\n                                        conf_threshold=conf,\n                                        nms_threshold=0.45)[0]\n\n        # ---- Ground Truth ----\n        gt_boxes = []\n        for obj in sample['objects']:\n            xmin, ymin, xmax, ymax = obj['bbox']\n            gt_boxes.append([xmin, ymin, xmax, ymax, 1.0, obj['class_idx']])\n\n        h, w = resized.shape[:2]\n\n        # ---- Draw GT ----\n        axes[row, 0].imshow(resized)\n        axes[row, 0].set_title(f\"Ground Truth | Index {idx}\")\n        for box in gt_boxes:\n            xmin, ymin, xmax, ymax, _, cls = box\n            cls = int(cls)\n            rect = patches.Rectangle(\n                (xmin * w, ymin * h),\n                (xmax - xmin) * w,\n                (ymax - ymin) * h,\n                linewidth=2, edgecolor='green', facecolor='none'\n            )\n            axes[row, 0].add_patch(rect)\n        axes[row, 0].axis('off')\n\n        # ---- Draw Prediction ----\n        axes[row, 1].imshow(resized)\n        axes[row, 1].set_title(\"Prediction\")\n        for box in pred_boxes:\n            xmin, ymin, xmax, ymax, score, cls = box\n            cls = int(cls)\n            rect = patches.Rectangle(\n                (xmin * w, ymin * h),\n                (xmax - xmin) * w,\n                (ymax - ymin) * h,\n                linewidth=2, edgecolor='red', facecolor='none'\n            )\n            axes[row, 1].add_patch(rect)\n            axes[row, 1].text(xmin * w, ymin * h - 4,\n                              f\"{CLASS_NAMES[cls]} {score:.2f}\",\n                              color='white', fontsize=8,\n                              bbox=dict(facecolor='red', alpha=0.7))\n        axes[row, 1].axis('off')\n\n    plt.tight_layout()\n    plt.savefig(\"comparison_results.png\", dpi=150, bbox_inches='tight')\n    plt.show()\n\nindices = [140,112,14,143,29,24,106,100,126,168,144,136,137]\ncompare_gt_vs_prediction_fixed(model, val_data, indices, conf=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}